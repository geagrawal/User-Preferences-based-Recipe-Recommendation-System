# -*- coding: utf-8 -*-
"""Recipes Test Unay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ePA6tH65lVslpk0EtKdTY91aWA8JYwR

### Install packages
"""

!pip install -q kaggle -q
!pip install spacy-transformers -q
!python -m spacy download en_core_web_lg -q

"""### Dataset Setup"""

from google.colab import drive
drive.mount('/content/drive')

kaggle_path: str = '/content/drive/MyDrive/Course Materials/4 Fall 23/CSE 258 Recommender Systems and Web Mining/Assignment/Assignment 2/kaggle.json'
!mkdir -p ~/.kaggle
!cp '/content/drive/MyDrive/Course Materials/4 Fall 23/CSE 258 Recommender Systems and Web Mining/Assignment/Assignment 2/kaggle.json' ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d shuyangli94/food-com-recipes-and-user-interactions

!mkdir -p dataset
!unzip -n food-com-recipes-and-user-interactions.zip -d dataset
# !rm food-com-recipes-and-user-interactions.zip

"""### Imports"""

import pickle
import pandas as pd
import os
import ast
import random
import numpy as np
from collections import defaultdict
import string
import re

from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import nltk
nltk.download('wordnet')

from tqdm import tqdm

import tensorflow as tf

BASE_PATH: str = './dataset'
INGREDIENTS_MAP = 'ingr_map.pkl'
RECIPES = 'PP_recipes.csv'
USERS = 'PP_users.csv'
RAW_INTERACTIONS = 'RAW_interactions.csv'
RAW_RECIPES = 'RAW_recipes.csv'
TRAIN, VAL, TEST = 'interactions_train.csv', 'interactions_validation.csv', 'interactions_test.csv'

partial_dataset = 300000
with open(os.path.join(BASE_PATH, INGREDIENTS_MAP), 'rb') as f:
    ingredients_map = pickle.load(f)
recipes = pd.read_csv(os.path.join(BASE_PATH, RECIPES))
users = pd.read_csv(os.path.join(BASE_PATH, USERS))
# raw_interactions = pd.read_csv(os.path.join(BASE_PATH, RAW_INTERACTIONS))[:partial_dataset]
# raw_recipes = pd.read_csv(os.path.join(BASE_PATH, RAW_RECIPES))
train, val, test = pd.read_csv(os.path.join(BASE_PATH, TRAIN)), pd.read_csv(os.path.join(BASE_PATH, VAL)), pd.read_csv(os.path.join(BASE_PATH, TEST))

"""Using only first `partial_dataset` entries and filtering out recipes made by less than 3 users"""

raw_interactions = pd.read_csv(os.path.join(BASE_PATH, RAW_INTERACTIONS))[:partial_dataset]
raw_recipes = pd.read_csv(os.path.join(BASE_PATH, RAW_RECIPES))

recipesPerUser = defaultdict(set[int])
usersPerRecipe = defaultdict(set[int])
ratings = {}

for i in tqdm(range(len(raw_interactions))):
    row = raw_interactions.iloc[i]
    user_id, recipe_id = row['user_id'], row['recipe_id']
    recipesPerUser[user_id].add(recipe_id)
    usersPerRecipe[recipe_id].add(user_id)
    ratings[(user_id, recipe_id)] = row['rating']

keepRecipes = {recipe for recipe, users in usersPerRecipe.items() if len(users) >= 2}
# Filter out these recipes from the raw_interactions DataFrame
raw_interactions = raw_interactions[raw_interactions['recipe_id'].isin(keepRecipes)]

# Filter out these recipes from the raw_recipes DataFrame
raw_recipes = raw_recipes[raw_recipes['id'].isin(keepRecipes)]

len(raw_interactions), len(raw_recipes)

"""### Visualizing data
u: User ID mapped to contiguous integer sequence from 0

i: Recipe ID mapped to contiguous integers from 0
"""

users.head()

recipes.head()

raw_interactions.head()

raw_recipes.head()

from collections import defaultdict
from tqdm import tqdm
unique_tags = defaultdict(int)
for tag in tqdm(raw_recipes['tags']):
  for t in ast.literal_eval(tag):
    unique_tags[t]+=1
print(unique_tags)

# for k, v in unique_tags.items():
#   print(k, v)
len(unique_tags)

from transformers import OpenAIGPTTokenizer

def convert_bp_encoded_fields(value: str) -> list[int]:
  return [int(x.strip('[] ')) for x in value.split(',')]

def get_pp_data(path: str) -> pd.DataFrame:
    tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')
    pp_recipes_df = pd.read_csv(path,
      converters={
        "name_tokens" : convert_bp_encoded_fields,
        "ingredient_tokens" : convert_bp_encoded_fields,
        "steps_tokens" : convert_bp_encoded_fields,
         })

tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')
[tokenizer.decode(ast.literal_eval(x)) for x in recipes['name_tokens'].iloc[:20].values]

print(recipes.columns)
print(users.columns)
print(raw_interactions.columns)
print(raw_recipes.columns)
print(train.columns)
print(test.columns)

"""## Importing data to use"""

import spacy

import spacy_transformers
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nlp=spacy.load("en_core_web_lg")

# reviews = []
# for review in raw_interactions.loc[raw_interactions['recipe_id'] == raw_interactions.iloc[0]['recipe_id']]['review']:
#     reviews.append(review)
# reviews_sim = []
# for i in range(len(reviews)):
#     r1 = nlp(reviews[i])
#     for row in tqdm(raw_interactions.iterrows()):
#         r2 = nlp(row[1].review)
#     reviews_sim.append([(r1.similarity(r2), row[1].recipe_id)])
#     reviews_sim.sort(reverse=True)

"""## Review Preprocessing"""

def get_aspects(x):
    doc=nlp(x) ## Tokenize and extract grammatical components
    doc=[i.text for i in doc if not i.is_stop and i.pos_=="NOUN"] ## Remove common words and retain only nouns
    doc=list(map(lambda i: i.lower(),doc)) ## Normalize text to lower case
    doc=pd.Series(doc)
    doc=doc.value_counts().head().index.tolist() ## Get 5 most frequent nouns
    return doc

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def tokenize_text(review_text):
    # Convert to lowercase
    review_text = review_text.lower()

    # Remove punctuation
    review_text = ' '.join(re.findall(r'\b\w+\b', review_text))
    review_text = review_text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize the text
    tokens = word_tokenize(review_text)

    # Remove stopwords
    # stop_words = set(stopwords.words('english'))

    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatize the tokens
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return tokens

clean_reviews = []
for i, r in tqdm(enumerate(raw_interactions['review'])):
    clean_reviews.append(tokenize_text(r) if isinstance(r, str) else [])
    if i == 10: break

for c, r in zip(clean_reviews[:11], raw_interactions.iloc[:11].review):
    print(c)
    print(r)
    print()

set(raw_interactions['user_id'])
c = 0
for u in users[users.n_items>10].u:
    print(train[train['u'] == u].iloc[0].user_id)
#   raw_interactions[raw_interactions['user_id']==126440]
# train[train['u'] == 3].iloc[0].user_id

# List users who have made at least X recipes
def filter_dataset_size(users, threshold):
    new_users = users[users.n_items>=threshold]
    return new_users
filter_dataset_size(users, 5)

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

nltk.download('punkt')  # Download the Punkt tokenizer models

def tokenize_recipe_steps(recipe_text):
    sentences = sent_tokenize(recipe_text)
    tokens = [word_tokenize(sentence) for sentence in sentences]
    return tokens

def simplify_tokens(tokens):
    simplified_tokens = []

    for sentence_tokens in tokens:
        simplified_sentence = [simplify_word(word) for word in sentence_tokens]
        simplified_tokens.append(simplified_sentence)

    return simplified_tokens

def simplify_word(word):
    # Define your simplification rules here
    word_mappings = {
        'chop': 'cut',
        'minced': 'finely chopped',
        # Add more mappings as needed
    }

    return word_mappings.get(word, word)

def simplify_tokens(tokens):
    simplified_tokens = []

    for sentence_tokens in tokens:
        simplified_sentence = [simplify_number(word) for word in sentence_tokens]
        simplified_tokens.append(simplified_sentence)

    return simplified_tokens

def simplify_number(word):
    if word.isdigit():
        # Replace numbers with symbols or words
        return 'NUM'
    else:
        return word

def reassemble_tokens(simplified_tokens):
    simplified_text = ' '.join([' '.join(sentence) for sentence in simplified_tokens])
    return simplified_text

recipe_text = "Chop the onions and garlic. Heat the oil in a pan. Add the onions and garlic and sautÃ© until golden brown."

tokens = tokenize_recipe_steps(recipe_text)
simplified_tokens = simplify_tokens(tokens)
simplified_text = reassemble_tokens(simplified_tokens)

print(simplified_text)

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

nltk.download('punkt')
nltk.download('stopwords')

def tokenize_recipe_steps(recipe_text):
    tokens = word_tokenize(recipe_text.lower())
    # Remove punctuation and stop words
    tokens = [word for word in tokens if word.isalpha() and word not in string.punctuation and word not in stopwords.words('english')]
    return tokens

def process_recipe_steps(recipe_data):
    processed_steps = []
    for steps in recipe_data['steps']:
        p = []
        for step in ast.literal_eval(steps):
            tokens = tokenize_recipe_steps(step)
            p.append(tokens)
        processed_steps.append(p)
    return processed_steps

# Process recipe steps
temp = process_recipe_steps(raw_recipes.iloc[:100])

# Print the processed DataFrame
for t, s in zip(temp, raw_recipes.iloc[:100].steps):
    print(t, s)

"""## Ingredients Preprocessing"""

# Combine ingredients
def count_all_ingredients(all_ingredients: list[list[str]]) -> dict:
    count_ingr = defaultdict(int)
    ingr_map = {}
    for tag in tqdm(raw_recipes['ingredients']):
        for t in ast.literal_eval(tag):
            count_ingr[t]+=1
    return count_ingr

def lemmatized_mapping(count_ingr: dict) -> dict:
    """
    Creates a map from ingredients in recipe to a lemmatized version
    """
    lemmatized_map = defaultdict(set[str])
    lemmatizer = WordNetLemmatizer()
    for k in tqdm(count_ingr):
        lemmatizedK = " ".join([lemmatizer.lemmatize(K) for K in k.split()])
        lemmatized_map[lemmatizedK].add(k)
    return lemmatized_map

def combine_ingredients(lemmatized_map: dict) -> dict:
    ingr_map = defaultdict(set[str])
    remove_ingr = set()
    lemmatizer = WordNetLemmatizer()
    for k1 in tqdm(lemmatized_map):
        # If ingredient is already marked for deletion, continue
        if k1 in remove_ingr: continue
        k1 = re.sub(r'[^a-zA-Z]', ' ', k1)
        lemmatizedK1 = lemmatizer.lemmatize(k1)
        # Iterate over all ingredients
        for k2 in lemmatized_map:
            if k1 in k2.split() or k1==k2:
                # Remove only if k1 != k2
                if k2!=k1:
                    remove_ingr.add(k2)
                # Add ingredient count
                ingr_map[lemmatizedK1].update(lemmatized_map[k2])
    # Remove ingredients from new_ingr
    for ingr in remove_ingr:
        if ingr in ingr_map:
            ingr_map.pop(ingr)
    return ingr_map

def count_new_ingredients(count_ingr: dict, ingr_map: dict) -> dict:
    new_count_ingr = defaultdict(int)
    all_ingr_map = defaultdict(list[str])
    for k, v in tqdm(ingr_map.items()):
        for i in v:
            new_count_ingr[k]+=count_ingr[i]
            all_ingr_map[i].append(k)
    return new_count_ingr, all_ingr_map

count_ingr = count_all_ingredients(list(raw_recipes['ingredients']))
lemmatized_map = lemmatized_mapping(count_ingr)
ingr_map = combine_ingredients(lemmatized_map)
new_count_ingr, all_ingr_map = count_new_ingredients(count_ingr, ingr_map)
ingrIDs = {ingr: i for i, ingr in enumerate(new_count_ingr)}

raw_recipes['ingredients_parsed'] = raw_recipes['ingredients'].apply(lambda ingr_list: [i for ingr in ast.literal_eval(ingr_list) for i in all_ingr_map[ingr]])
raw_recipes['ingredients_parsed_ids'] = raw_recipes['ingredients_parsed'].apply(lambda ingr_list: [ingrIDs[ingr] for ingr in ingr_list])

print(len(ingr_map), len(count_ingr.keys()))
for i in range(5):
    print(ast.literal_eval(raw_recipes.iloc[i].ingredients))
    print(list(raw_recipes.iloc[i].ingredients_parsed))
    print()

"""## Generate Negative Tests"""

allRecipes = list(set(raw_recipes['id']))
train_split, val_split, test_split = 0.60, 0.15, 0.25
train_split, val_split, test_split = int(train_split*len(raw_interactions)), int(val_split*len(raw_interactions)), int(test_split*len(raw_interactions))
train_interactions = raw_interactions[:train_split]
val_interactions = raw_interactions[train_split:val_split+train_split]
test_interactions = raw_interactions[train_split+val_split:]

def get_mappings(interactions, recipe_id_col: str = 'recipe_id', user_id_col: str = 'user_id', rating_col: str = 'rating'):
    recipesPerUser = defaultdict(set[int])
    usersPerRecipe = defaultdict(set[int])
    ratings = {}

    for i in tqdm(range(len(interactions))):
        row = interactions.iloc[i]
        user_id, recipe_id = row[user_id_col], row[recipe_id_col]
        recipesPerUser[user_id].add(recipe_id)
        usersPerRecipe[recipe_id].add(user_id)
        ratings[(user_id, recipe_id)] = row[rating_col]
    return recipesPerUser, usersPerRecipe, ratings

recipesPerUser, usersPerRecipe, ratings = get_mappings(train_interactions)

"""Ingredients per recipe and per user"""

ingrPerRecipe = defaultdict(set[str])
for i in tqdm(range(len(raw_recipes))):
    row = raw_recipes.iloc[i]
    ingrPerRecipe[row['id']] = row['ingredients_parsed_ids']

ingrPerUser = defaultdict(set[str])
usersPerIngr = defaultdict(set[str])
for u, recipesSet in tqdm(recipesPerUser.items()):
    for r in recipesSet:
        ingrPerUser[u].update(ingrPerRecipe[r])

for r, users in tqdm(usersPerRecipe.items()):
    ingr = ingrPerRecipe[r]
    for i in ingr:
        usersPerIngr[i].update(users)

"""Tags, Nutrition and Prep Time Hashing"""

def normalize(mat):
    norm = np.linalg.norm(mat, axis=0)
    return mat/norm

tagsPerUser = defaultdict(set[str])
tagsPerRecipe = defaultdict(set[str])
tagsPerRecipeIDs = defaultdict(set[str])
nutritionPerRecipe = defaultdict(list[int])
timePerRecipe = defaultdict(list[int])


for i in tqdm(range(len(raw_recipes))):
    row = raw_recipes.iloc[i]
    tagsPerRecipe[row['id']].update(set(ast.literal_eval(row['tags'])))
    nutritionPerRecipe[row['id']] = np.array(ast.literal_eval(row['nutrition']))
    timePerRecipe[row['id']] = row['minutes']

tagIDs = {tag: i for i, tag in enumerate({tag for tags in tagsPerRecipe.values() for tag in tags})}
for i in tqdm(range(len(raw_recipes))):
    row = raw_recipes.iloc[i]
    tagsPerRecipeIDs[row['id']].update({tagIDs[t] for t in set(ast.literal_eval(row['tags']))})

for u, recipes in tqdm(recipesPerUser.items()):
    allTags = set()
    for r in recipes:
        allTags.update(tagsPerRecipe[r])
    tagsPerUser[u] = allTags

# nutritionNormalize = []
# timeNormalize = []
# for r in nutritionPerRecipe:
#     nutritionNormalize.append(nutritionPerRecipe[r])
#     timeNormalize.append(timePerRecipe[r])
# nutritionNormalize = normalize(np.array(nutritionNormalize))
# timeNormalize = normalize(np.array(timeNormalize))

# for i in tqdm(range(len(raw_recipes))):
#     row = raw_recipes.iloc[i]
#     nutritionPerRecipe[row['id']] = nutritionNormalize[i]
#     timePerRecipe[row['id']] = timeNormalize[i]

# Given a user with all the recipes that they have tried, select recipes that they have not tried and use them to create a negative dataset
def generate_negative_pairs(train_interactions, val_interactions, test_interactions,
                            allRecipes: list[str], recipesPerUser: dict,
                            recipe_id_col: str = 'recipe_id', user_id_col: str = 'user_id'):
    train_pairs = []
    for i in tqdm(range(len(train_interactions))):
        row = train_interactions.iloc[i]
        train_pairs.append([row[user_id_col], row[recipe_id_col], 1])

    val_pairs = []
    test_pairs = []

    for i in tqdm(range(len(val_interactions))):
        row = val_interactions.iloc[i]
        user_id = row[user_id_col]
        negative_recipe = random.choice(allRecipes)
        while negative_recipe in recipesPerUser[user_id]:
            negative_recipe = random.choice(allRecipes)
        val_pairs.append([user_id, negative_recipe, 0])
        val_pairs.append([user_id, row[recipe_id_col], 1])
    random.shuffle(val_pairs)

    for i in tqdm(range(len(test_interactions))):
        row = test_interactions.iloc[i]
        user_id = row[user_id_col]
        negative_recipe = random.choice(allRecipes)
        while negative_recipe in recipesPerUser[user_id]:
            negative_recipe = random.choice(allRecipes)
        test_pairs.append([user_id, negative_recipe, 0])
        test_pairs.append([user_id, row[recipe_id_col], 1])
    random.shuffle(test_pairs)
    return train_pairs, val_pairs, test_pairs
train_pairs, val_pairs, test_pairs = generate_negative_pairs(train_interactions, val_interactions, test_interactions, allRecipes, recipesPerUser)

X_train, y_train = [[u, r] for u, r, _ in train_pairs], [y for _, _, y in train_pairs]
X_val, y_val = [[u, r] for u, r, _ in val_pairs], [y for _, _, y in val_pairs]
X_test, y_test = [[u, r] for u, r, _ in test_pairs], [y for _, _, y in test_pairs]
recipeIDs = {r:i for i, r in enumerate(allRecipes)}
userIDs = {u:i for i, u in enumerate(recipesPerUser.keys())}

"""### BPR"""

class BPRbatch(tf.keras.Model):
    def __init__(self, K, lamb):
        super(BPRbatch, self).__init__()
        # Initialize variables
        self.betaI = tf.Variable(tf.random.normal([len(allRecipes)],stddev=0.001))
        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))
        self.gammaI = tf.Variable(tf.random.normal([len(recipeIDs),K],stddev=0.001))
        # Regularization coefficient
        self.lamb = lamb

    # Prediction for a single instance
    def predict(self, u, i):
        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)
        return p

    # Regularizer
    def reg(self):
        return self.lamb * (tf.nn.l2_loss(self.betaI) +\
                            tf.nn.l2_loss(self.gammaU) +\
                            tf.nn.l2_loss(self.gammaI))

    def score(self, sampleU, sampleI):
        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)
        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)
        beta_i = tf.nn.embedding_lookup(self.betaI, i)
        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)
        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)
        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)
        return x_ui

    def call(self, sampleU, sampleI, sampleJ):
        x_ui = self.score(sampleU, sampleI)
        x_uj = self.score(sampleU, sampleJ)
        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))

def trainingStepBPR(model, interactions):
    Nsamples = 100000
    with tf.GradientTape() as tape:
        sampleU, sampleI, sampleJ = [], [], []
        for _ in range(Nsamples):
            u,i = random.choice(interactions) # positive sample
            j = random.choice(allRecipes) # negative sample
            while j in recipesPerUser[u]:
                j = random.choice(allRecipes)
            sampleU.append(userIDs[u])
            sampleI.append(recipeIDs[i])
            sampleJ.append(recipeIDs[j])

        loss = model(sampleU,sampleI,sampleJ)
        loss += model.reg()
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients((grad, var) for
                              (grad, var) in zip(gradients, model.trainable_variables)
                              if grad is not None)
    return loss.numpy()

optimizer = tf.keras.optimizers.Adam(0.1)
modelBPR = BPRbatch(10, 0.00001)
for i in range(100):
    obj = trainingStepBPR(modelBPR, X_train)
    if (i % 10 == 9): print("iteration " + str(i+1) + ", objective = " + str(obj))

def AUCu(u, interactions, N, model):
    win = 0
    if N > len(interactions[u]):
        N = len(interactions[u])
    positive = random.sample(interactions[u],N)
    negative = random.sample(allRecipes.difference(interactions[u]),N)
    for i,j in zip(positive,negative):
        si = model.predict(userIDs[u], recipeIDs[i]).numpy()
        sj = model.predict(userIDs[u], recipeIDs[j]).numpy()
        if si > sj:
            win += 1
    return win/N

def AUC(interactions, model):
    av = []
    for u in tqdm(interactions):
        av.append(AUCu(u, interactions, 10, model))
    return sum(av) / len(av)
allRecipes = set(allRecipes)
interactionsTestPerUser = defaultdict(set)
itemSet = set()
for i in tqdm(range(len(val_interactions))):
    row = val_interactions.iloc[i]
    interactionsTestPerUser[row['user_id']].add(row['recipe_id'])
    itemSet.add(row['recipe_id'])
print(AUC(interactionsTestPerUser, modelBPR))
interactionsTestPerUser = defaultdict(set)
itemSet = set()
for i in tqdm(range(len(test_interactions))):
    row = test_interactions.iloc[i]
    interactionsTestPerUser[row['user_id']].add(row['recipe_id'])
    itemSet.add(row['recipe_id'])
print(AUC(interactionsTestPerUser, modelBPR))
allRecipes = list(allRecipes)

def accuracy(y_pred, y, t):
    return sum((y_pred>t)!=y)/len(y)

y_pred_val = np.array([modelBPR.predict(userIDs[u],recipeIDs[i]).numpy() for u,i in tqdm(X_val)])
best_threshold = 0
best_acc = 0
for i in tqdm(np.arange(-1, 2, 0.01)):
    acc = accuracy(y_pred_val, y_val, i)
    if acc>best_acc:
        best_acc= acc
        best_threshold = i
print(best_threshold, best_acc)

y_pred = np.array([modelBPR.predict(userIDs[u],recipeIDs[i]).numpy() for u,i in tqdm(X_test)])
print(accuracy(np.array(y_pred), y_test, best_threshold))

"""Trying new models

### BPR With Additional Features
"""

class BPRbatchAdditional(tf.keras.Model):
    def __init__(self, K, lamb, num_users, num_recipes, num_ingr):
        super(BPRbatchAdditional, self).__init__()
        # Initialize variables for users and recipes
        nutrition_size = 100
        self.betaI = tf.Variable(tf.random.normal([num_recipes], stddev=0.001))
        self.gammaU = tf.Variable(tf.random.normal([num_users, K], stddev=0.001))
        self.gammaI = tf.Variable(tf.random.normal([num_recipes, K], stddev=0.001))

        # Initialize variables for additional features
        self.gammaCalories = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaTotalFat = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaSugar = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaSodium = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaProtein = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaSaturatedFat = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaCarbohydrates = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaIngr = tf.Variable(tf.random.normal([num_ingr, K], stddev=0.001))

        # Regularization coefficient
        self.lamb = lamb

    def predict(self, u, i, nutrition, ingr):
        gamma_ingr = tf.reduce_mean(tf.nn.embedding_lookup(self.gammaIngr, ingr), axis=0)
        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1) +\
        tf.tensordot(self.gammaU[i], self.gammaProtein[nutrition[4]], 1) +\
        tf.tensordot(self.gammaU[i], self.gammaCalories[nutrition[0]], 1) +\
        tf.tensordot(self.gammaU[i], self.gammaCarbohydrates[nutrition[0]], 1) +\
        tf.tensordot(self.gammaU[i], gamma_ingr, 1)
        return p

    def reg(self):
        # Regularization for all embeddings
        return self.lamb * (tf.nn.l2_loss(self.betaI) +
                            tf.nn.l2_loss(self.gammaU) +
                            tf.nn.l2_loss(self.gammaI) +
                            tf.nn.l2_loss(self.gammaCalories) +
                            tf.nn.l2_loss(self.gammaTotalFat) +
                            tf.nn.l2_loss(self.gammaSugar) +
                            tf.nn.l2_loss(self.gammaSodium) +
                            tf.nn.l2_loss(self.gammaProtein) +
                            tf.nn.l2_loss(self.gammaSaturatedFat) +
                            tf.nn.l2_loss(self.gammaCarbohydrates))

    def score(self, sampleU, sampleI, nutrition, ingr):
        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)
        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)
        nutrition = tf.convert_to_tensor(nutrition, dtype=tf.int32)
        beta_i = tf.nn.embedding_lookup(self.betaI, i)
        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)
        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)
        gamma_ingr = tf.convert_to_tensor([tf.reduce_sum(tf.nn.embedding_lookup(self.gammaIngr, i2), axis=0) for i2 in ingr])
        gamma_calories = tf.nn.embedding_lookup(self.gammaCalories, nutrition[:, 0])
        gamma_total_fat = tf.nn.embedding_lookup(self.gammaTotalFat, nutrition[:, 1])
        gamma_sugar = tf.nn.embedding_lookup(self.gammaSugar, nutrition[:, 2])
        gamma_sodium = tf.nn.embedding_lookup(self.gammaSodium, nutrition[:, 3])
        gamma_protein = tf.nn.embedding_lookup(self.gammaProtein, nutrition[:, 4])
        gamma_saturated_fat = tf.nn.embedding_lookup(self.gammaSaturatedFat, nutrition[:, 5])
        gamma_carbohydrates = tf.nn.embedding_lookup(self.gammaCarbohydrates, nutrition[:, 6])
        return beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i) +
                                      tf.multiply(gamma_u, gamma_protein) +
                                      tf.multiply(gamma_u, gamma_calories) +
                                      tf.multiply(gamma_u, gamma_carbohydrates) +
                                      tf.multiply(gamma_u, gamma_ingr), 1)

    def call(self, sampleU, sampleI, sampleJ, nutritionI, nutritionJ, ingrI, ingrJ):
        x_ui = self.score(sampleU, sampleI, nutritionI, ingrI)
        x_uj = self.score(sampleU, sampleJ, nutritionJ, ingrJ)
        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))

def trainingStepBPRAdditional(model, interactions):
    Nsamples = 1000
    with tf.GradientTape() as tape:
        sampleU, sampleI, sampleJ = [], [], []
        nutritionI, nutritionJ = [], []
        ingrI, ingrJ = [], []
        for _ in range(Nsamples):
            u,i = random.choice(interactions) # positive sample
            j = random.choice(allRecipes) # negative sample
            while j in recipesPerUser[u]:
                j = random.choice(allRecipes)
            sampleU.append(userIDs[u])
            sampleI.append(recipeIDs[i])
            sampleJ.append(recipeIDs[j])
            # nutritionI.append(nutritionPerRecipe[i])
            # nutritionJ.append(nutritionPerRecipe[j])
            nutritionI.append((np.clip(nutritionPerRecipe[i], None, 99)).astype(np.int32))
            nutritionJ.append((np.clip(nutritionPerRecipe[j], None, 99)).astype(np.int32))
            ingrI.append(tagIDs[t] for t in tagsPerRecipe[i])
            ingrJ.append(tagIDs[t] for t in tagsPerRecipe[j])

        loss = model(sampleU,sampleI,sampleJ, nutritionI, nutritionJ, ingrI, ingrJ)
        loss += model.reg()
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients((grad, var) for
                              (grad, var) in zip(gradients, model.trainable_variables)
                              if grad is not None)
    return loss.numpy()
optimizer = tf.keras.optimizers.Adam(0.1)
modelBPRAdditional = BPRbatchAdditional(K=10, lamb=0.0001, num_users=len(userIDs), num_recipes=len(recipeIDs), num_ingr=len(tagIDs))
for i in range(100):
    obj = trainingStepBPRAdditional(modelBPRAdditional, X_train)
    if (i % 10 == 9): print("iteration " + str(i+1) + ", objective = " + str(obj))

len(tagIDs)

def accuracy(y_pred, y, t):
    print(y_pred>t)
    return sum((y_pred>t)!=y)/len(y)
y_pred_val = np.array([modelBPRAdditional.predict(userIDs[u],recipeIDs[i],(np.clip(nutritionPerRecipe[i], None, 99)).astype(np.int32), ingrPerRecipe[i]).numpy() for u,i in tqdm(X_val)])
# y_pred_val2 = tf.reduce_sum(y_pred_val, 1)
best_threshold = 0
best_acc = 0
for i in tqdm(np.arange(-1, 2, 0.01)):
    acc = accuracy(y_pred_val2, np.array(y_val), i)
    if acc>best_acc:
        best_acc= acc
        best_threshold = i
print(best_threshold, best_acc)

class BPRbatchAdditional(tf.keras.Model):
    def __init__(self, K, lamb, num_users, nutrition_size, num_recipes):
        super(BPRbatchAdditional, self).__init__()
        # Initialize variables for users and recipes
        self.betaI = tf.Variable(tf.random.normal([num_recipes], stddev=0.001))
        self.gammaU = tf.Variable(tf.random.normal([num_users, K], stddev=0.001))
        self.gammaI = tf.Variable(tf.random.normal([num_recipes, K], stddev=0.001))

        # Initialize variables for additional features
        self.gammaCalories = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaTotalFat = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaSugar = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaSodium = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaProtein = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaSaturatedFat = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        self.gammaCarbohydrates = tf.Variable(tf.random.normal([nutrition_size, K], stddev=0.001))
        # self.gammaTags = tf.Variable(tf.random.normal([num_unique_tags, K], stddev=0.001))
        self.gammaMinutes = tf.Variable(tf.random.normal([60, K], stddev=0.001))

        # Regularization coefficient
        self.lamb = lamb

    def get_tags_embedding(self, tags):
        # Aggregate tag embeddings for a recipe
        tag_embeddings = tf.nn.embedding_lookup(self.gammaTags, tags)
        return tf.reduce_mean(tag_embeddings, axis=0)

    def predict(self, u, i, nutrition, minutes):
        user_embed = self.gammaU[u]
        item_embed = self.gammaI[i]
        nutrition_embed = self.gammaCalories[nutrition[0]] + \
                    self.gammaTotalFat[nutrition[1]] + \
                    self.gammaSugar[nutrition[2]] + \
                    self.gammaSodium[nutrition[3]] + \
                    self.gammaProtein[nutrition[4]] + \
                    self.gammaSaturatedFat[nutrition[5]] + \
                    self.gammaCarbohydrates[nutrition[6]]
        # tags_embed = self.get_tags_embedding(tags)
        minutes_embed = self.gammaMinutes[minutes]
        # Summing up all embeddings
        return self.betaI[i] + tf.tensordot(user_embed + nutrition_embed + minutes_embed, item_embed, 1)

    def reg(self):
        # Regularization for all embeddings
        return self.lamb * (tf.nn.l2_loss(self.betaI) +
                            tf.nn.l2_loss(self.gammaU) +
                            tf.nn.l2_loss(self.gammaI) +
                            tf.nn.l2_loss(self.gammaCalories) +
                            tf.nn.l2_loss(self.gammaTotalFat) +
                            tf.nn.l2_loss(self.gammaSugar) +
                            tf.nn.l2_loss(self.gammaSodium) +
                            tf.nn.l2_loss(self.gammaProtein) +
                            tf.nn.l2_loss(self.gammaSaturatedFat) +
                            tf.nn.l2_loss(self.gammaCarbohydrates) +
                            tf.nn.l2_loss(self.gammaMinutes))

    def score(self, sampleU, sampleI, nutrition, minutes):
        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)
        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)
        print(u.shape)
        return self.predict(u, i, nutrition, minutes)

    def call(self, sampleU, sampleI, sampleJ, nutritionI, nutritionJ, minutesI, minutesJ):
        x_ui = self.score(sampleU, sampleI, nutritionI, minutesI)
        x_uj = self.score(sampleU, sampleJ, nutritionJ, minutesJ)
        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))

def trainingStepBPRAdditional(model, interactions):
    Nsamples = 100000
    with tf.GradientTape() as tape:
        sampleU, sampleI, sampleJ = [], [], []
        nutritionI, nutritionJ, tagsI, tagsJ, minutesI, minutesJ = [], [], [], [], [], []
        for _ in range(Nsamples):
            u,i = random.choice(interactions) # positive sample
            j = random.choice(allRecipes) # negative sample
            while j in recipesPerUser[u]:
                j = random.choice(allRecipes)
            sampleU.append(userIDs[u])
            sampleI.append(recipeIDs[i])
            sampleJ.append(recipeIDs[j])
            nutritionI.append((np.clip(nutritionPerRecipe[i]*10000, None, 100)).astype(np.int32))
            nutritionJ.append((np.clip(nutritionPerRecipe[j]*10000, None, 100)).astype(np.int32))
            # tagsI.append([tagIDs[t] for t in tagsPerRecipe[i]])
            # tagsJ.append([tagIDs[t] for t in tagsPerRecipe[j]])
            minutesI.append((timePerRecipe[i]*60).astype(np.int32))
            minutesI.append((timePerRecipe[j]*60).astype(np.int32))

        loss = model(sampleU,sampleI,sampleJ, nutritionI, nutritionJ, minutesI, minutesJ)
        loss += model.reg()
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients((grad, var) for
                              (grad, var) in zip(gradients, model.trainable_variables)
                              if grad is not None)
    return loss.numpy()

optimizer = tf.keras.optimizers.Adam(0.1)
modelBPRAdditional = BPRbatchAdditional(K=10, lamb=0.0001, num_users=len(userIDs), nutrition_size=1000, num_recipes=len(recipeIDs))
for i in range(100):
    obj = trainingStepBPRAdditional(modelBPRAdditional, X_train)
    if (i % 10 == 9): print("iteration " + str(i+1) + ", objective = " + str(obj))

def accuracy(y_pred, y, t):
    return sum((y_pred>t)!=y)/len(y)
y_pred_val = np.array([modelBPRAdditional.predict(userIDs[u],recipeIDs[i], \
 (nutritionPerRecipe[i]*100).astype(np.int32), [tagIDs[t] \
                                                for t in tagsPerRecipe[i]], \
                                                 (timePerRecipe[i]*60).astype(np.int32)).numpy() \
                       for u,i in tqdm(X_val)])
best_threshold = 0
best_acc = 0
for i in np.arange(-1, 2, 0.01):
    acc = accuracy(y_pred_val, y_val, i)
    if acc>best_acc:
        best_acc= acc
        best_threshold = i
print(best_threshold, best_acc)

y_pred = np.array([modelBPRAdditional.predict(userIDs[u],recipeIDs[i], np.hstack([nutritionPerRecipe[i], timePerRecipe[i]])).numpy() for u,i in X_test])
print(accuracy(np.array(y_pred), y_test, best_threshold))

"""### Jaccard"""

def Jaccard(s1, s2):
    numer = len(s1.intersection(s2))
    denom = len(s1.union(s2))
    if denom == 0:
        return 0
    return numer / denom

"""User-user Similarity by Ingredients
* Given a user-recipe pair
* For a given user, I have all the recipes they have cooked
* For those recipes, I have all ingredients
* So I have user-ingredients map
* With this, a user-user mapping can be constructed using ingredient similarity
* For similar users, a list of recipes tried can be constructed
* If current recipe is in the list, it will be cooked

User-user Similarity by Recipes
* Given a user-recipe pair
* For a given user, I have all the recipes they have cooked
* With this, a user-user mapping can be constructed using recipes similarity
* For similar users, a list of recipes tried can be constructed
* If current recipe is in the list, it will be cooked

"""

def similarItems(item, N, itemsPerUser, usersPerItem):
    similarities = []
    users = usersPerItem[item]
    allItemsByUsers = {r for u in users for r in itemsPerUser[u]}
    for r in allItemsByUsers:
        if r == item: continue
        similarities.append([Jaccard(users, usersPerItem[r]), r])
    similarities.sort(reverse=True)
    return similarities[:N]

def similarUsers(user, N, itemsPerUser, usersPerItem):
    similarUsers = []
    items = itemsPerUser[user]
    allUsersPerItem = {u for i in items for u in usersPerItem[i]}
    for u in allUsersPerItem:
        if u==user: continue
        similarUsers.append([Jaccard(itemsPerUser[u], items), u])
    similarUsers.sort(reverse=True)
    return similarUsers[:N]

similarRecipesByUsers = {}
similarUsersByRecipe = {}
for r in tqdm(usersPerRecipe):
    similarRecipesByUsers[r] = similarItems(r, 5000, recipesPerUser, usersPerRecipe)
for u in tqdm(recipesPerUser):
    similarUsersByRecipe[u] = similarUsers(u, 5000, recipesPerUser, usersPerRecipe)

for r in tqdm(usersPerRecipe):
    similarRecipesByUsers[r] = [s[1] for s in similarRecipesByUsers[r]]
for u in tqdm(recipesPerUser):
    similarUsersByRecipe[u] = [s[1] for s in similarUsersByRecipe[u]]

tags_val_sim = []
for u, r in X_val:
    tags_val_sim.append(Jaccard(tagsPerUser[u], tagsPerRecipe[r]))

tags_val_sim = np.array(tags_val_sim)
best_threshold_tags = 0
best_acc = 0
for i in tqdm(np.arange(0.1, 2, 0.01)):
    acc = accuracy(tags_val_sim, y_val, i)
    if acc>best_acc:
        best_acc= acc
        best_threshold_tags = i
print(best_threshold_tags, best_acc)

tags_test_sim = []
for u, r in X_test:
    temp = Jaccard(tagsPerUser[u], tagsPerRecipe[r])
    tags_test_sim.append(temp if temp else 1)
# tags_test_sim = np.array([Jaccard(tagsPerUser[u], tagsPerRecipe[r]) for u,i in tqdm(X_test)])
print(accuracy(tags_test_sim, y_test, best_threshold))

"""Nutrition and Time"""

userNutrition = defaultdict(lambda: np.zeros(7))
userMinutes = defaultdict(float)
for u, recipes in tqdm(recipesPerUser.items()):
    for r in recipes:
        userNutrition[u]+=nutritionPerRecipe[r]
        userMinutes[u] += timePerRecipe[r]
    if len(recipes):
        userNutrition[u]/=len(recipes)
        userMinutes[u]/=len(recipes)

from sklearn.linear_model import LogisticRegression

def distance(train_data, test_data):
    return np.abs(train_data-test_data)

def normalize(mat):
    norm = np.linalg.norm(mat, axis=0)
    return mat/norm

# X_train_nutrition = np.array([distance(userNutrition[u], nutritionPerRecipe[r]) for u, r in X_train])
X_val_nutrition = np.array([distance(userNutrition[u], nutritionPerRecipe[r]) for u, r in X_val])
X_test_nutrition = np.array([distance(userNutrition[u], nutritionPerRecipe[r]) for u, r in X_test])
X_train_minutes = np.array([distance(userMinutes[u], timePerRecipe[r]) for u, r in X_train])
X_val_minutes = np.array([distance(userMinutes[u], timePerRecipe[r]) for u, r in X_val])
X_test_minutes = np.array([distance(userMinutes[u], timePerRecipe[r]) for u, r in X_test])

# X_train_nutrition_minutes = np.concatenate((X_train_nutrition, X_train_minutes[:, None]), axis=1)
X_val_nutrition_minutes = np.concatenate((X_val_nutrition, X_val_minutes[:, None]), axis=1)
X_test_nutrition_minutes = np.concatenate((X_test_nutrition, X_test_minutes[:, None]), axis=1)

X_val_nutrition_minutes = normalize(X_val_nutrition_minutes)
X_test_nutrition_minutes = normalize(X_test_nutrition_minutes)

logisticRegression = LogisticRegression(fit_intercept=False)
logisticRegression.fit(X_val_nutrition_minutes[:, [1, 2, 3, 5, 7]], y_val)

y_test_nutrition_minutes = logisticRegression.predict(X_test_nutrition_minutes[:, [1, 2, 3, 5, 7]])
print(np.sum(y_test_nutrition_minutes==y_test)/len(y_test), logisticRegression.coef_, logisticRegression.intercept_)

"""### Process test data"""

pred_jac = []
for u, r in tqdm(X_val):
    maxSim = 0


    # ingr = set(ingrPerRecipe[r])
    # for r2 in recipesPerUser[u]:
    #     if r2!=r:
    #         maxSim = max(Jaccard(ingr, ingrPerRecipe[r2]), maxSim)
    # pred_jac.append(maxSim)

    # users = usersPerRecipe[r]
    # for r2 in recipesPerUser[u]:
    #     if r2!=r:
    #         maxSim = max(Jaccard(users, usersPerRecipe[r2]), maxSim)
    # pred_jac.append(maxSim)
    # pred_jac.append(Jaccard(set(ingrPerRecipe[r]), set(ingrPerUser[u])))

def accuracy(y_pred, y, t):
    # print(y_pred>t)
    return sum((y_pred>t)!=y)/len(y)
pred_jac = 1-np.array(pred_jac)
best_threshold = 0
best_acc = 0
for i in tqdm(np.arange(-1, 2, 0.01)):
    acc = accuracy(pred_jac, y_val, i)
    if acc>best_acc:
        best_acc= acc
        best_threshold = i
print(best_threshold, best_acc)

pred_jac

"""### TF-IDF of Ingredients"""

from sklearn.feature_extraction.text import TfidfVectorizer

ingrTFIDF = defaultdict(str)
ingrTFIDF = [" ".join(i) for i in raw_recipes['ingredients_parsed']]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(ingrTFIDF)

feature_names = vectorizer.get_feature_names_out()
tfidf_array = tfidf_matrix.toarray()
tfidf_dict = {feature_names[i]: tfidf_array[:, i] for i in range(len(feature_names))}

ingr_val_sim = []
for u, r in X_val:
    temp = Jaccard(ingrPerUser[u], ingrPerRecipe[r])
    ingr_val_sim.append(temp if temp else 1)

ingr_val_sim = np.array(ingr_val_sim)
best_threshold_sim = 0
best_acc = 0
for i in tqdm(np.arange(-1, 2, 0.01)):
    acc = accuracy(ingr_val_sim, y_val, i)
    if acc>best_acc:
        best_acc= acc
        best_threshold_sim = i
print(best_threshold_sim, best_acc)

ingr_test_sim = np.array([Jaccard(ingrPerUser[u], ingrPerRecipe[r]) for u,i in tqdm(X_test)])
print(accuracy(ingr_test_sim, y_test, best_threshold))